% ================================================================================================= %
\documentclass[12pt,A4]{article}

\title
{
   CS 290 Paper Commentaries
}

\author
{
   Ren Trista A. de la Cruz
}

\date
{
   \today
}

% ================================================================================================= %

\begin{document}

\maketitle

% ================================================================================================= %

\section*{Computing with Spikes}

\emph{Computing with Spikes} \cite{maass-2002-comp-spike} gives a quick overview of the idea behind 
computing models based on \emph{spiking neurons} and the (then) current research that the author
Wolfgang Maass and his colleagues were conducting. A significant portion of the paper is dedicated
to describing the mechanisms of a spiking neuron.

The field that studies spiking neuron models (or spiking neural networks) is in the intersection of 
the field of neuroscience and the field of theoretical computer science. Unlike abstract models of 
computation like Turing  machines and counter machines in theoretical computer science, spiking 
neuron models are significantly more complex since they are used to model and study the
computational process of the nervous system. Spiking neuron models should be abstract enough for 
them to be used as models of computation that can solve abstract problems (in theoretical computer
science) but they should be detailed enough so that they can be used to explain phenomena in the
nervous system (in neuroscience).

Artificial neural networks are computing models that are `inspired' biological neural network but the 
activation mechanism (using real functions like sigmoid and trigonometric functions) of the neuron 
in these models does not represent how activation occurs in a biological neuron. The activation 
mechanism in a spiking neuron model more closely represents the activation mechanism of a biological 
neuron. Aside from the difference in activation mechanism, the organization of an artificial neural 
network is also different to the organization of a spiking neural network.  

Maass et al.'s research involves determining the computational function of \emph{neural 
microcircuits}. In a spiking neuron model, these microcircuits are model using spiking neurons. The 
research asks how exactly does a spiking neuron work. The research involves studying actual
biological neurons in order to observe how they produce and process spikes. Other aspects of their 
research involve the study of how the spiking neurons are organized (into networks), how memory are
stored in the networks, and how learning is done by the networks. 

A large part of the paper describes the spiking neuron and gives a general idea behind the spiking 
mechanism. The neuron has three main parts: the \emph{soma}, the \emph{dendritic tree}, and the
\emph{axonal tree}. The soma is the body of the neuron that produces the signal called 
\emph{spikes}. The dendritic tree is the `input' region of the neuron where it receives signals from 
other neurons. The axonal tree is the `output' region of the neuron where it sends out signals to 
other neurons. A part of the axonal tree (output region) of one neuron can be `connected' to a part 
of the dendritic tree (input region) of another neuron. This `connection' or interface between a 
neuron's axonal tree and another neuron's dendritic tree is called a \emph{synapse}.  

A neuron has a \emph{membrane potential}, a voltage value based on the difference between the charge
inside and the charge outside the membrane of the neuron. A neuron has a resting membrane potential 
which is about $-70$ millivolts. The `spike' in a spiking neuron model refers to an \emph{action 
potential} in a neuron. An action potential is a sudden increase (around $40$ millivolts) and then a 
sudden decrease of the neuron's membrane potential (happens in less than $3$ milliseconds). The term 
`spike' refers to that even of the membrane potential spiking. 

When a neuron receives a certain combination of inputs (spikes from other neurons received by the 
dendritic tree), it will produce a spike. There is threshold mechanism in a neuron. If the 
`combination' of input spikes passes a certain threshold, the neuron will spike. The amplitude of 
the spike does not change for a neuron. The input spikes can dictate if the neuron will spike or not 
but not how `large' the spike is. A series of input spikes can, however, dictate the timing (i.e.
frequency) in which the neuron spikes.

When a neuron spikes, the spike travels along the axon then reaches the axon terminals. The axon
terminals can be connected to dendrites of other neurons. The synapse, that connects the axon 
terminal to dendrite of another neuron, is responsible from `processing' the spike and then passing 
a `processed' spike to the next neuron. The synapse has an internal state/configuration. This state
is affected by the spikes it receives. In a sense, this is a form of memory. This state affects how
the synapse processed incoming spikes.   

In the biological neuron, the spiking event is a result of a combination of electrochemical 
activities that involve the neuron membrane, voltage-gate ion channels, potassium and sodium ions, 
etc. The activities in a synapse involve neurotransmitters, neurotransmitter transporters and
receptors, etc. In the spiking neuron models, these electrochemical activities are abstracted and
simplified in order to have manageable models of computation but they are still somewhat faithful to 
their biological analogues (at least much more faithful compared to artificial neural network 
models). 

A spiking neuron can only produce a spike, a single type of signal. A neuron can not produce a 
`large' spike or `small' spike. All spikes produced are identical. Information in spiking neuron
models are encoded by spiking patterns (in time) produced by a neuron. This pattern in known as
a \emph{spike train}. For example, a spike can be represented by `$1$' and no spike represented by
`$0$'. A spiking neuron's activities (spiking or resting) can be represented by a string (the spike
train) over the binary alphabet $\{0,1\}$. This string is a pattern in time instead of being a
pattern in space. One can only observe the spike train by looking at the activities of a neuron for
a certain period of time.

In summary, a spiking neuron model is  a model of computation based on the spiking mechanism of a
biological neuron. It is much more complex than other models of computation because it also has the
role of modelling neural phenomena. Spiking neural network s are much more similar to biological 
neural network unlike artificial neural networks.

% ================================================================================================= %

\section*{Third Generation Neural Networks: Spiking Neural Networks}


\emph{Third Generation Neural Networks: Spiking Neural Networks} \cite{ghosh-dastidar-2009-snn} 
gives an overview of a spiking neural network model which a \emph{third generation} neural network.

Neural networks can be categorized in to \emph{generations}: the \emph{first}, the \emph{second}, 
and the \emph{third} generation. The categorization is both historical and technical. On the 
historical side, the generation simply refers to the order in which the neural network is
invented/introduced. First generation neural networks came first, followed by the second generation,
then followed by the third generation. On the technical side, the paper focuses on the activation 
mechanism of the neurons in order to differentiate the neural networks from different generations.

The first generation networks use neurons that have an \emph{integrate-and-fire with threshold} 
mechanism. The first generation neuron has an internal state that is the weighted sum of the input
signals from other neurons. The integration process refers to the computation of the weighted sum
(internal state). If the internal state passes a given threshold value for the neuron, then the 
neuron fires a signal. This process is the `firing' process that is conditional on the internal 
state and the threshold value. The second generation networks use neurons that have an 
\emph{integrate-and-activate} mechanism. Similar to the first generation, a neuron computes the
weighted sum of the input signals. The weighted sum is then feed as input to an \emph{activation
function}. The activation function can be a hyperbolic function, a sigmoid (logistic) function, 
a binary step function (similar to the threshold mechanism in first generation neuron), etc. The
output of the activation function will be the output of the neuron. Unlike first generation 
neurons with a single (discrete) type of signal, the output of second generation neurons is 
continuous. If the activation function used by a second generation network is continuous and
differentiable, then the \emph{back-propagation} algorithm can be used to train the network on 
labelled data. 

The first and second generation networks have a lot in common with each other. In fact, the first
generation networks are a special case of the second generation networks. Both generations have
the integration process of computing the weighted sum and they both use activation functions. First
generation networks specifically use the binary step function while the second generation networks
can use any activation functions. Additionally, both first and second generation networks are 
synchronized. This means at every step in the computation, all neurons in the network perform 
integration followed by function activation at the same time. All neurons produce output signals
at the same time. 

Third generation networks use \emph{spiking} neurons. A spiking neuron is similar to a first 
generation neuron in two ways. It outputs a single type of signal called a \emph{spike} and a spike 
is produced if the internal state of the spiking neuron reaches a specified threshold. What 
differentiate third generation networks with the first and the second generation networks are the
definition of the internal state of a neuron and the timing of the firing of neurons in the network.

In a spiking neuron, the internal state is more of a continuous value that changes in time and is
affected the incoming input spikes. In a spiking neural network (third generation), neurons are
not synchronized. A spiking neuron can fire any time as long as its internal state passes the 
threshold specified for the neuron.  For example, neuron $A$ and neuron $B$ are connected to neuron
$C$ which means they send the spikes they generate to neuron $C$. It is possible that in a given 
time period, both neuron $A$ and neuron $B$ send one spike each to neuron $C$ but at different 
points in time. The arrival times of input spikes from neuron $A$ and neuron $B$ affect the resulting
internal state of neuron $C$. The resulting internal state of neuron $C$ after receiving both
spikes at the same time will be different to the resulting internal state if the spikes arrived at
different times. The resulting internal state of neuron $C$ after receiving a spike from neuron $A$
at time $t$ and a spike from neuron $B$ at time $t+d$ will be different to the resulting internal 
state of neuron $C$ if it received neuron $A$'s spike at time $t$ and neuron $B$'s spike at time $t+d'$
(where $d \neq d'$). The timing of spikes are important in a spiking neural network. In general, in
a given time period, we can look at the outputs of neuron $A$ and neuron $B$ as sequences of
spikes. These sequences are called \emph{spikes trains}. A spike train is a pattern of spikes
in time.

Third generation networks do not all fall under one model. There are different spiking neural 
network models. Different models may have different mechanisms for the spiking neuron. They can also
have different ways of connecting the neurons and building the network. Different spiking neuron 
models have different ways of defining how sets of spike train inputs affect the internal state of
a neuron. It is possible to have very detailed spiking mechanism that models the behavior of a
biological spiking neuron. Some spiking neuron models use a system of differential equations to 
define how the internal state changes with respect to input spike trains. These kind of models are
computationally intensive to simulate. i.e. The computer is essentially solving differential 
equations for each neuron in the networks. Such models are better from a neuroscience perspective 
but worse for a computational perspective. Spiking neural networks in used in computer science 
primarily use a much simpler spiking neuron model. Additionally, it is easier to adapt the back
propagation algorithm to simpler spiking neuron models.

The paper presents a particular spiking neural network model where, similar to a common 
feed-forward second generation neural network, neurons are arranged into layers and each neuron
in one layer is connected to all neurons in the next layer. The difference is that, in this 
particular spiking neural network model, the `connection' from one neuron to another neuron in the
next layer is actually a set of $K$ connection from one neuron to the next. For example, if the model
has $k=5$, when neuron $A$ is connected to neuron $B$ then there are $k=5$ connections from neuron 
$A$ to neuron $B$. Each connection has its own weight and delay. When neuron $A$ sends a spike, 
a copy of the spike will go through the five connections.Initially, the output of neuron $A$ will
appear as five separate spike trains which are possibly different from each other due to different
delays in the connections. One can look at these spike trains as discrete (square) waves. In each
of the connection, there is a \emph{synapse} that transforms the discrete wave into a more smooth
(continuous) wave like a sine wave. The synapse also `amplifies' this smooth wave by the weight 
associated with the connection. The discrete wave (spike train) is called the \emph{pre-synaptic 
potential} while the continuous wave is called the \emph{post-synaptic potential}. One can combine
these post-synaptic potentials (continuous waves) from all connections into a single continuous wave. 
In summary, one can look at the output of a neuron as set of spike trains that are transformed 
into continuous waves and combined into a single continuous wave.

A neuron will receive multiple continuous waves (inputs) from all neurons in the previous layer. The
integration performed by the neuron is the process of combining all these continuous wave inputs. 
If some of the peaks in the combined wave input passes the threshold specified in the neuron, then
the neuron will send a spike out. For a combined continuous wave input, the spiking neuron will 
produce a spike train output.

Both input (continuous wave, post-synaptic potential) and output (discrete wave, spike train) of a
spiking neuron are waves which means they are pattern in time. One can look at spiking neural 
networks as another form of generalization. i.e the generalization of the second generation 
networks by allowing input and output as sequences (in the form of waves) instead of single values
(in the second generation network, weighted sum is the input and the function result is output). In
this case, the second generation networks will appear as spiking neural networks that exclusively
deal with sequences with single elements (single value input and output).

Spiking neural networks are particularly effective when it comes to learning time-dependent patterns
(time series). They can also be use to create smaller networks (fewer neurons/layers) if one can 
effectively encoding both input and output as patterns in time. For example, in a second generation
neural network for image processing, all features (pixel values) are fed to the network at the same
time. If the image is large, the network that processes the image will also be large. By encoding parts of
the images (sets of pixels) as sequences, one can create a smaller spiking neural network that can
deal with these sequences as input and produced the corresponding output sequences. Finding the
time-based encoding for both input and output is not a trivial task.

One disadvantage of using spiking neural network is its computationally intensive training. There
are a lot of parameters (different weights and delays of the connections) the training algorithm 
needs to adjust. The error surface of a spiking neural network is also highly uneven. i.e. A slight
change in the delay/weight of one of the connections from one neuron to next neuron can result in a
disproportionate change in the output spike train.

% ================================================================================================= %

\section*{Network of Spiking Neurons: Third Generation Neural Network Models}

\cite{maass-1997-third}

% ================================================================================================= %

\section*{NEXT}

% ================================================================================================= %

\bibliographystyle{plain}
\bibliography{cs-290-reviews}

\end{document}


























